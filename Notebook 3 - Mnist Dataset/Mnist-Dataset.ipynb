{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mnist Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image of Sample Mnist Dataset](https://camo.githubusercontent.com/2eefe56bdacb4d28015c435d0801abf00b95b95f/68747470733a2f2f636f72706f637261742e636f6d2f77702d636f6e74656e742f75706c6f6164732f323031342f31302f6669677572655f312e706e67)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the Mnist Dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST database of handwritten digits, available from this page, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image.\n",
    "It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting. (Source:http://yann.lecun.com/exdb/mnist/)\n",
    "\n",
    "*The files from the dataset are also available from the website above but I have them saved here on github for easy access*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bytes"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Because the file is compressed (.gz) we have to unzip it we do this using a python library called gzip\n",
    "\n",
    "import gzip\n",
    "\n",
    "with gzip.open('Dataset/t10k-images-idx3-ubyte.gz','rb') as f:        \n",
    "    file_content = f.read()\n",
    "    \n",
    "# If we do a simple type() function we can see what is file_content now\n",
    "\n",
    "type(file_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 16 bytes of an image file allow us to retrieve information about the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2051\n",
      "10000\n",
      "28\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "print(int.from_bytes(file_content[0:4], byteorder='big'))\n",
    "print(int.from_bytes(file_content[4:8], byteorder='big'))\n",
    "print(int.from_bytes(file_content[8:12], byteorder='big'))\n",
    "print(int.from_bytes(file_content[12:16], byteorder='big'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These four bits of information actually tell us a lot about the file.\n",
    "\n",
    "First 2051 tells us that it is an unsigned int.\n",
    "\n",
    "The 1000 tells us the amount of images in the file.\n",
    "\n",
    "Both the 28's tell us that there is 28 rows of pixels and 28 columns of pixels which should be the case for the image size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in a single image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read in a single image file we just have to skip the first 16 bytes in the file_contents array and then go to 800 as the size of the image is 28x28 which is 784."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255\n",
      "  255 255 255 255 255 255 255 255 255 255]\n",
      " [255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255\n",
      "  255 255 255 255 255 255 255 255 255 255]\n",
      " [255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255\n",
      "  255 255 255 255 255 255 255 255 255 255]\n",
      " [255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255\n",
      "  255 255 255 255 255 255 255 255 255 255]\n",
      " [255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255\n",
      "  255 255 255 255 255 255 255 255 255 255]\n",
      " [255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255\n",
      "  255 255 255 255 255 255 255 255 255 255]\n",
      " [255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255\n",
      "  255 255 255 255 255 255 255 255 255 255]\n",
      " [255 255 255 255 255 255 171  70  96 104 195 219 255 255 255 255 255 255\n",
      "  255 255 255 255 255 255 255 255 255 255]\n",
      " [255 255 255 255 255 255  33   1   1   1   1  14  57  57  57  57  57  57\n",
      "   57  57  85 203 255 255 255 255 255 255]\n",
      " [255 255 255 255 255 255 188 141 183 141  92  28   1  30   1   1   1   5\n",
      "   26   1   1 115 255 255 255 255 255 255]\n",
      " [255 255 255 255 255 255 255 255 255 255 255 238 189 241 188 188 188 196\n",
      "  234  19   1 149 255 255 255 255 255 255]\n",
      " [255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255\n",
      "  172   2  46 237 255 255 255 255 255 255]\n",
      " [255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 233\n",
      "   22   0 172 255 255 255 255 255 255 255]\n",
      " [255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 126\n",
      "    1  17 211 255 255 255 255 255 255 255]\n",
      " [255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 196   6\n",
      "    1 193 255 255 255 255 255 255 255 255]\n",
      " [255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 122   1\n",
      "   68 250 255 255 255 255 255 255 255 255]\n",
      " [255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 246  50   7\n",
      "  197 255 255 255 255 255 255 255 255 255]\n",
      " [255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 129   1  73\n",
      "  255 255 255 255 255 255 255 255 255 255]\n",
      " [255 255 255 255 255 255 255 255 255 255 255 255 255 255 180   4  15 198\n",
      "  255 255 255 255 255 255 255 255 255 255]\n",
      " [255 255 255 255 255 255 255 255 255 255 255 255 255 236  34   1  89 255\n",
      "  255 255 255 255 255 255 255 255 255 255]\n",
      " [255 255 255 255 255 255 255 255 255 255 255 255 252  52   1  36 220 255\n",
      "  255 255 255 255 255 255 255 255 255 255]\n",
      " [255 255 255 255 255 255 255 255 255 255 255 255 217   1   1 178 255 255\n",
      "  255 255 255 255 255 255 255 255 255 255]\n",
      " [255 255 255 255 255 255 255 255 255 255 255 224  31   1 140 254 255 255\n",
      "  255 255 255 255 255 255 255 255 255 255]\n",
      " [255 255 255 255 255 255 255 255 255 255 255 122   1   1 203 255 255 255\n",
      "  255 255 255 255 255 255 255 255 255 255]\n",
      " [255 255 255 255 255 255 255 255 255 255 194  13   1   1 203 255 255 255\n",
      "  255 255 255 255 255 255 255 255 255 255]\n",
      " [255 255 255 255 255 255 255 255 255 255 134   1   1  36 215 255 255 255\n",
      "  255 255 255 255 255 255 255 255 255 255]\n",
      " [255 255 255 255 255 255 255 255 255 255 134   1  48 237 255 255 255 255\n",
      "  255 255 255 255 255 255 255 255 255 255]\n",
      " [255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255\n",
      "  255 255 255 255 255 255 255 255 255 255]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x20a804c7358>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## get the image details from the file_content index 16-800 , reshape it back into a picture (28x28)\n",
    "image = ~np.array(list(file_content[16:800])).reshape(28,28).astype(np.uint8)\n",
    "\n",
    "image2 = ~np.array(list(file_content[800:1584])).reshape(28,28).astype(np.uint8)\n",
    "\n",
    "## As we can see here the image is simply displayed as a 2-D array of pixels, but we can use plt.imshow() to actually \n",
    "## bring the pixels to life\n",
    "print(image)\n",
    "\n",
    "plt.imshow(image, cmap='gray')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x20a807b8208>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## The second image in the file\n",
    "\n",
    "plt.imshow(image2, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in a single label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in a label is just as simple, but we just need to skip the first 8 bytes instead of 16 because it is just a 1D array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "## Import our label training set\n",
    "with gzip.open('Dataset/t10k-labels-idx1-ubyte.gz', 'rb') as f:\n",
    "    labels = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int.from_bytes(labels[8:9], byteorder=\"big\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int.from_bytes(labels[9:10], byteorder=\"big\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving our images locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 ## For image/file writing\n",
    "import gzip\n",
    "## First we need to read in our data , we do this once again using gzip\n",
    "\n",
    "with gzip.open('dataset/t10k-images-idx3-ubyte.gz', 'rb') as file_images:\n",
    "        image_contents = file_images.read()\n",
    "    \n",
    "with gzip.open('dataset/t10k-labels-idx1-ubyte.gz', 'rb') as file_labels:\n",
    "        labels_contents = file_labels.read()\n",
    "        \n",
    "## Next we need to create variables to control our position in the for loop\n",
    "## We need two for each because its done using splicing e.g testimages[control1:control2]\n",
    "\n",
    "## for images \n",
    "trainImagesSplice1=16 ## Ignore magic numbers \n",
    "trainImagesSplice2=800 ## 784 is the length of each picture\n",
    "\n",
    "## for labels \n",
    "trainLabelsSplice1=8 ## Ignore magic numbers\n",
    "trainLabelsSplice2=9 ## each label is just 1\n",
    "\n",
    "for x in range(10):\n",
    "        images = ~np.array(list(image_contents[trainImagesSplice1:trainImagesSplice2])).reshape(28,28).astype(np.uint8)\n",
    "        labels = np.array(list(labels_contents[trainLabelsSplice1:trainLabelsSplice2])).astype(np.uint8)\n",
    "        \n",
    "        # Increment by 784 to get next image\n",
    "        trainImagesSplice1+=784\n",
    "        trainImagesSplice2+=784\n",
    "        \n",
    "        # Increment by 1 to get label\n",
    "        trainLabelsSplice1+=1\n",
    "        trainLabelsSplice2+=1\n",
    "        \n",
    "        # Write the image file at every iteration of the for loop\n",
    "        cv2.imwrite('ImageDigit-(' + str(labels) +')' + '.png', image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building our neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import keras.\n",
    "import keras as kr\n",
    "from keras.models import Sequential\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# Start a neural network, building it by layers.\n",
    "model = kr.models.Sequential()\n",
    "\n",
    "# Add a hidden layer with 1000 neurons and an input layer with 784.\n",
    "model.add(Dense(512, input_shape=(784,)))\n",
    "model.add(Activation('relu'))                            \n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# Build the graph.\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('dataset/train-images-idx3-ubyte.gz', 'rb') as f:\n",
    "    train_img = f.read()\n",
    "\n",
    "with gzip.open('dataset/train-labels-idx1-ubyte.gz', 'rb') as f:\n",
    "    train_lbl = f.read()\n",
    "    \n",
    "train_img =  np.array(list(train_img[16:])).reshape(60000, 28, 28).astype(np.uint8)/ 255.0\n",
    "train_lbl =  np.array(list(train_lbl[ 8:])).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = train_img.reshape(60000, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 [0 0 0 0 0 1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# For encoding categorical variables.\n",
    "import sklearn.preprocessing as pre\n",
    "\n",
    "encoder = pre.LabelBinarizer()\n",
    "encoder.fit(train_lbl)\n",
    "outputs = encoder.transform(train_lbl)\n",
    "\n",
    "print(train_lbl[0], outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 19s 323us/step - loss: 0.2494 - acc: 0.9258\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 19s 315us/step - loss: 0.1027 - acc: 0.9686\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 19s 316us/step - loss: 0.0720 - acc: 0.9780\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 19s 320us/step - loss: 0.0554 - acc: 0.9823\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 19s 318us/step - loss: 0.0436 - acc: 0.9860\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 19s 319us/step - loss: 0.0409 - acc: 0.9866\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 19s 318us/step - loss: 0.0349 - acc: 0.9884\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 19s 322us/step - loss: 0.0300 - acc: 0.9896\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 20s 339us/step - loss: 0.0283 - acc: 0.9905\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 19s 320us/step - loss: 0.0254 - acc: 0.9915\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20a8aeb2b38>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(inputs, outputs, epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in our test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('dataset/t10k-images-idx3-ubyte.gz', 'rb') as f:\n",
    "    test_img = f.read()\n",
    "\n",
    "with gzip.open('dataset/t10k-labels-idx1-ubyte.gz', 'rb') as f:\n",
    "    test_lbl = f.read()\n",
    "    \n",
    "## Reshape our image \n",
    "\n",
    "##    all images in the training set have an range from 0-1\n",
    "##    and not from 0-255 so we divide our flatten images\n",
    "##    (a one dimensional vector with our 784 pixels)\n",
    "##    to use the same 0-1 based range\n",
    "test_img =  np.array(list(test_img[16:])).reshape(10000, 784).astype(np.uint8) / 255.0\n",
    "test_lbl =  np.array(list(test_lbl[ 8:])).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9811"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Compare our predictions vs actual results\n",
    "(encoder.inverse_transform(model.predict(test_img)) == test_lbl).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.8791665e-11, 4.9437304e-10, 1.2571307e-08, 1.3608143e-06,\n",
       "        2.7961134e-06, 7.6993575e-07, 1.4379147e-14, 2.8240484e-06,\n",
       "        1.6090404e-10, 9.9999225e-01]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Test our model\n",
    "model.predict(test_img[7:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=uint8)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## We can use encoder.inverse_transform() to reverse our binary transform to get the number it thinks it is\n",
    "encoder.inverse_transform(model.predict(test_img[14:15]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x20a8c10d9e8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## As we can see here our model correctly predicted the image\n",
    "plt.imshow(test_img[14].reshape(28, 28), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
